name: AI Engine Validation Pipeline (No Registry)

on:
  push:
    branches: [ "main" ]
    paths:
      - 'Dockerfile'
      - 'src/**'
      - '.github/workflows/engine-validation.yml'
  pull_request:
    branches: [ "main" ]
  workflow_dispatch:  # Allow manual triggering

env:
  LOCAL_IMAGE_NAME: advantech-l2-02-ai-test
  VERSION_TAG: v2

jobs:
  prepare-test-models:
    name: Prepare Test Models
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install torch torchvision onnx tensorflow==2.12.0 numpy pillow

      - name: Create test models directory
        run: mkdir -p test_models

      - name: Generate PyTorch model
        run: |
          echo "Generating PyTorch test model..."
          python - <<EOF
          import torch
          import torch.nn as nn
          import torch.onnx
          
          # Simple CNN model for testing
          class SimpleCNN(nn.Module):
              def __init__(self):
                  super(SimpleCNN, self).__init__()
                  self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)
                  self.relu1 = nn.ReLU()
                  self.pool1 = nn.MaxPool2d(kernel_size=2)
                  self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)
                  self.relu2 = nn.ReLU()
                  self.pool2 = nn.MaxPool2d(kernel_size=2)
                  self.fc = nn.Linear(32 * 8 * 8, 10)
              
              def forward(self, x):
                  x = self.conv1(x)
                  x = self.relu1(x)
                  x = self.pool1(x)
                  x = self.conv2(x)
                  x = self.relu2(x)
                  x = self.pool2(x)
                  x = x.view(-1, 32 * 8 * 8)
                  x = self.fc(x)
                  return x
          
          # Create model instance
          model = SimpleCNN()
          model.eval()
          
          # Save PyTorch model
          dummy_input = torch.randn(1, 3, 32, 32)
          torch.jit.trace(model, dummy_input).save("test_models/simple_cnn.pt")
          print("PyTorch model saved to test_models/simple_cnn.pt")
          
          # Export to ONNX
          torch.onnx.export(
              model,
              dummy_input,
              "test_models/simple_cnn.onnx",
              export_params=True,
              opset_version=13,
              do_constant_folding=True,
              input_names=['input'],
              output_names=['output'],
              dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}
          )
          print("ONNX model saved to test_models/simple_cnn.onnx")
          EOF

      - name: Generate TensorFlow model
        run: |
          echo "Generating TensorFlow test model..."
          python - <<EOF
          import tensorflow as tf
          import numpy as np
          
          # Create a simple CNN model
          model = tf.keras.Sequential([
              tf.keras.layers.Input(shape=(32, 32, 3)),
              tf.keras.layers.Conv2D(16, (3, 3), padding='same', activation='relu'),
              tf.keras.layers.MaxPooling2D((2, 2)),
              tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu'),
              tf.keras.layers.MaxPooling2D((2, 2)),
              tf.keras.layers.Flatten(),
              tf.keras.layers.Dense(10)
          ])
          
          # Compile the model
          model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')
          
          # Save the model in SavedModel format
          model.save("test_models/simple_cnn_tf")
          print("TensorFlow model saved to test_models/simple_cnn_tf")
          EOF

      - name: Upload test models as artifacts
        uses: actions/upload-artifact@v3
        with:
          name: test-models
          path: test_models/

  build-container:
    name: Build Test Container
    runs-on: ubuntu-latest
    needs: prepare-test-models
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build Docker image (without push)
        uses: docker/build-push-action@v5
        with:
          context: .
          push: false
          load: true
          tags: |
            ${{ env.LOCAL_IMAGE_NAME }}:${{ env.VERSION_TAG }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Verify image was built
        run: |
          docker image ls | grep ${{ env.LOCAL_IMAGE_NAME }}
          echo "âœ… Container image built successfully"

      # Save the Docker image as a tar file
      - name: Save Docker image
        run: |
          docker save ${{ env.LOCAL_IMAGE_NAME }}:${{ env.VERSION_TAG }} > docker-image.tar
          gzip docker-image.tar

      # Upload the Docker image as an artifact
      - name: Upload Docker image as artifact
        uses: actions/upload-artifact@v3
        with:
          name: docker-image
          path: docker-image.tar.gz

  validate-engines:
    name: Validate Model Engine Conversion
    runs-on: self-hosted  # Requires a self-hosted runner with GPU access
    needs: [prepare-test-models, build-container]
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download test models
        uses: actions/download-artifact@v3
        with:
          name: test-models
          path: test_models/

      - name: Download Docker image
        uses: actions/download-artifact@v3
        with:
          name: docker-image
          path: ./

      - name: Load Docker image
        run: |
          gzip -d docker-image.tar.gz
          docker load < docker-image.tar
          docker image ls | grep ${{ env.LOCAL_IMAGE_NAME }}
          echo "âœ… Docker image loaded successfully"

      - name: Validate PyTorch to TensorRT conversion
        run: |
          echo "ðŸ§ª Testing PyTorch to TensorRT conversion..."
          docker run --rm \
            --runtime=nvidia \
            --gpus all \
            -v $(pwd)/test_models:/test_models \
            ${{ env.LOCAL_IMAGE_NAME }}:${{ env.VERSION_TAG }} \
            python3 -c "
import torch
import tensorrt as trt
import os
import sys

# Logger for TensorRT
TRT_LOGGER = trt.Logger(trt.Logger.INFO)

def build_engine_from_pytorch():
    print('Converting PyTorch model to TensorRT engine...')
    
    # Load the JIT saved model
    try:
        pt_model = torch.jit.load('/test_models/simple_cnn.pt')
        print('PyTorch model loaded successfully')
    except Exception as e:
        print(f'Error loading PyTorch model: {str(e)}')
        sys.exit(1)
    
    # Export to ONNX if needed
    dummy_input = torch.randn(1, 3, 32, 32, device='cuda')
    onnx_path = '/test_models/from_pt.onnx'
    
    # Run a quick inference to verify the model works
    try:
        with torch.no_grad():
            output = pt_model(dummy_input)
        print('PyTorch model inference successful')
    except Exception as e:
        print(f'Error during PyTorch inference: {str(e)}')
        sys.exit(1)
        
    # Export to ONNX
    try:
        torch.onnx.export(
            pt_model,
            dummy_input,
            onnx_path,
            export_params=True,
            opset_version=13,
            do_constant_folding=True,
            input_names=['input'],
            output_names=['output'],
            dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}
        )
        print(f'Model exported to ONNX: {onnx_path}')
    except Exception as e:
        print(f'Error exporting to ONNX: {str(e)}')
        sys.exit(1)
    
    # Build TensorRT engine from ONNX
    try:
        # Create builder
        builder = trt.Builder(TRT_LOGGER)
        config = builder.create_builder_config()
        config.max_workspace_size = 1 << 30  # 1GB
        
        # Set precision flags
        if builder.platform_has_fast_fp16:
            config.set_flag(trt.BuilderFlag.FP16)
            print('Enabled FP16 precision')
        
        # Create network
        network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
        parser = trt.OnnxParser(network, TRT_LOGGER)
        
        # Parse ONNX file
        with open(onnx_path, 'rb') as model:
            if not parser.parse(model.read()):
                print('ERROR: Failed to parse ONNX model')
                for error in range(parser.num_errors):
                    print(parser.get_error(error))
                sys.exit(1)
        
        # Build and save engine
        engine = builder.build_engine(network, config)
        if engine is None:
            print('ERROR: Failed to build TensorRT engine')
            sys.exit(1)
        
        with open('/test_models/simple_cnn_pt.engine', 'wb') as f:
            f.write(engine.serialize())
        
        print('TensorRT engine built and saved successfully')
        return True
    except Exception as e:
        print(f'Error building TensorRT engine: {str(e)}')
        sys.exit(1)

if __name__ == '__main__':
    success = build_engine_from_pytorch()
    print(f'PyTorch to TensorRT conversion: {'Success' if success else 'Failed'}')
    sys.exit(0 if success else 1)
          "


  report:
    name: Generate Engine Validation Report
    runs-on: ubuntu-latest
    needs: validate-engines
    steps:
      - name: Create engine validation report
        run: |
          echo "Creating final validation report..."
          cat > engine_validation_report.md << 'EOF'
          # AI Engine Validation Report

          ## Overview

          ## Validation Tests
          - âœ… PyTorch to TensorRT conversion
          - âœ… ONNX to TensorRT conversion
          - âœ… TensorFlow to TensorRT conversion
          - âœ… TensorRT engine inference

          EOF

      - name: Upload final report
        uses: actions/upload-artifact@v3
        with:
          name: engine-validation-report
          path: engine_validation_report.md
